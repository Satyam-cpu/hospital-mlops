https://hospital-mlops.onrender.com
ğŸ¥ Hospital Inpatient Charges â€“ End-to-End MLOps Project
ğŸ“Œ Project Overview

This project demonstrates an end-to-end MLOps pipeline for predicting Hospital Inpatient Charges and monitoring the model after deployment.

The focus is not only model building, but also:

Deployment

Logging

Monitoring

Data drift detection

Reproducibility

The project is built keeping real-world production constraints (free tier, dependency conflicts, data drift) in mind.

ğŸ¯ Objective

Build a machine learning model to predict Average Total Payments for hospital inpatient services.

Deploy the model as a FastAPI service.

Log predictions for monitoring.

Detect data drift using Evidently AI.

Demonstrate a complete MLOps lifecycle using free and practical tools.

ğŸ§  Problem Statement

Hospitals generate large volumes of inpatient billing data.
Over time, pricing patterns change, which can reduce model accuracy.

This project answers:

How to deploy an ML model?

How to log predictions?

How to detect when the data distribution changes?

When should the model be retrained?

ğŸ—‚ï¸ Project Structure
Hospital Charges for Inpatients/
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ main.py                  # FastAPI application
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ inpatientCharges.csv     # Training dataset
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ inpatient_model.pkl      # Trained ML model
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ predictions.csv          # Runtime logs (ignored in git)
â”‚   â””â”€â”€ predictions_sample.csv   # Sample logs for GitHub
â”‚
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ drift_report.py           # Drift detection script
â”‚   â””â”€â”€ drift_report.html         # Generated drift report
â”‚
â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ swagger_predict.png
â”‚   â”œâ”€â”€ local_logs.png
â”‚   â”œâ”€â”€ drift_report.png
â”‚   â””â”€â”€ docker_running.png
â”‚
â”œâ”€â”€ requirements.txt              # Pinned dependencies
â”œâ”€â”€ Dockerfile                    # Docker configuration
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

âš™ï¸ Tech Stack

Python 3.10

Pandas, NumPy, Scikit-learn

FastAPI + Uvicorn

Docker

Evidently AI (Drift Detection)

GitHub

Render (Deployment demo)

ğŸ”„ End-to-End Workflow
Data â†’ Model Training â†’ API Deployment
     â†’ Prediction Logging
     â†’ Drift Detection
     â†’ Retraining Decision

Step-by-step:

Train ML model on historical inpatient data.

Save trained model.

Serve predictions via FastAPI.

Log prediction inputs & outputs.

Compare training vs production data.

Detect data drift.

Decide when to retrain.

ğŸš€ How to Run the Project (Local)
1ï¸âƒ£ Create & Activate Virtual Environment
py -3.10 -m venv venv
venv\Scripts\Activate.ps1

2ï¸âƒ£ Install Dependencies
python -m pip install --upgrade pip
pip install -r requirements.txt

3ï¸âƒ£ Run FastAPI App (Local)
uvicorn app.main:app --reload


Open Swagger UI:

http://localhost:8000/docs

4ï¸âƒ£ Make Predictions

Use /predict endpoint from Swagger.

Predictions are logged into:

logs/predictions.csv

ğŸ³ Run Using Docker (Recommended)
Build Docker Image
docker build -t hospital-mlops .

Run with Volume (for logging)
docker run -p 8000:8000 -v ${PWD}\logs:/app/logs hospital-mlops


Swagger:

http://localhost:8000/docs

ğŸ“Š Monitoring & Drift Detection
Generate Drift Report
python monitoring/drift_report.py


This generates:

monitoring/drift_report.html

What is Checked?

Feature distribution change

PSI (Population Stability Index)

Statistical tests

ğŸ“ˆ Drift Interpretation Rules
PSI Value	Meaning	Action
< 0.10	No Drift	No action
0.10 â€“ 0.25	Mild Drift	Monitor
â‰¥ 0.25	Severe Drift	Retrain
ğŸ” Retraining Decision Logic

Retrain the model when:

Key feature shows PSI â‰¥ 0.25

Multiple features drift

Business metrics degrade

Periodic retraining (3â€“6 months)

â˜ï¸ Deployment (Render)

Dockerized app deployed on Render (Free Tier).

Due to free-tier limitations:

Persistent logging is demonstrated locally.

Cloud deployment is used for API demo only.

âš ï¸ Limitations (Transparent & Honest)

Render free tier does not support free persistent disks.

Logs shown via local Docker volumes.

Monitoring demonstrated offline with Evidently.

ğŸ§  Key Learnings

Dependency pinning is critical in MLOps.

Containers are stateless by default.

Monitoring is as important as training.

Drift detection prevents silent model failure.

ğŸ§¾ Resume-Ready Highlights

Built and deployed an end-to-end MLOps pipeline.

Implemented prediction logging and drift detection.

Used Evidently AI for monitoring.

Handled real-world dependency conflicts.

Dockerized and deployed ML service.

ğŸ“Œ Conclusion

This project demonstrates real-world MLOps practices using practical and mostly free tools.
It focuses on reliability, monitoring, and reproducibility, not just model accuracy.
